---
title: "Benchmark"
---

This document describe how the benchmarking and performance testing of Ribasim is handled. In Ribasim, the benchmarking includes and regression tests on the testmodels and regressive performent tests on the production models.

The idea of regression tests on the testmodels is to run models with various solvers, run models with a sparse Jacobian and a dense one and compare the outputs.
It will possibly involve production model in the future.
And runtime performance tests is lined up for next step (in [issue #1698](https://github.com/Deltares/Ribasim/issues/1698)).

The idea of regressive performance tests on the production models is to test the performance of running the production models.
It will report if the new changes of the code decrease the performance of the model or result in failed runs.

# Benchmarking of the test models
## Benchmark the ODE solvers
The benchmarking of the ODE solvers is done by running the test models with different ODE solvers and solver settings and compare the output with the benchmark.

The settings include the true and false of jacobian density and auto differentiation. Currently, 4 model is chosen to undergo the regression tests. They are `trivial`, `basic`, `pid_control` and `subnetwork_with_sources`.

The current benchmark is the output files of a stable run of the test models with the default (`QNDF`) solver, with default settings.
The output files `basin.arrow` and `flow.arrow` are used for comparison.
Different margins are set for the comparison of the outputs, and the benchmark is considered passed if the output is within the margin.
Since we are still in the process of evaluating the performance of different solvers, the margin is subject to change.

The regression tests are run on a weekly basis.

## Benchmark the rest of the test models
TODO, and can be merged with the section [benchmark of the ODE solvers](#benchmark-the-ode-solvers) when it is done.

## Benchmark of the rest of ODE solvers
TODO, [#issue 1658](https://github.com/Deltares/Ribasim/issues/1658), and can be merged with the section [benchmark of the ODE solvers](#benchmark-the-ode-solvers) when it is done.

# Benchmarking of the production model
Regressive performance tests on the production models are done by running the production models with the new changes of the code and compare the runtime performance with the benchmark.
The current benchmark is the output files of a stable run of the production models with the default (`QNDF`) solver.
The output file "basin_state.arrow" which records the end states of the basin is used for comparison.
Since the development of the model is still ongoing, the benchmark is subject to change.

The regressive performance tests are currently run on a weekly basis. When the development of the model is more stable, the tests will be run more frequently.
